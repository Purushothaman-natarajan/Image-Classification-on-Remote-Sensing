# -*- coding: utf-8 -*-
"""Classification Model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1thHIn_fB3YSpjQA03ASjgFFR5HG64Wn-

## **Satellite Image Classification Model**

**Author-1 : Purushothaman Natarajan**

**Author-2 : Kamal Basha**

Source Dataset : Kaggle

Dataset Name : AID: A scene classification dataset

Link : https://www.kaggle.com/datasets/jiayuanchengala/aid-scene-classification-datasets

Approach: Transfer learning for Image classification using pre-trained models on ImageNet Dataset.
"""

from google.colab import drive
drive.mount('/content/drive')

# Unzip the dataset folder in the path

# Run this block once

import zipfile
import os
def unzip_folder(zip_path, extract_path):
  with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall(extract_path)

# Example usage:
zip_path = '/content/drive/MyDrive/RS LLM/Classification Model/Dataset/AID Dataset.zip'
extract_path = '/content/drive/MyDrive/RS LLM/Classification Model/Dataset'
unzip_folder(zip_path, extract_path)


# Install necessary Libraries:
!pip install opencv-python
!pip install tensorflow==2.15
!pip install keras==2.15.0
!pip install scikit-learn
!pip install pandas
!pip install numpy
!pip install seaborn

#print the current directory
print(os.getcwd())

# Load the images and labels

import os
from PIL import Image

images = []
labels = []

master_data_path = "/content/drive/MyDrive/RS LLM/Classification Model/Dataset/AID"

def load_images_and_labels(folder_path, label):
    for filename in os.listdir(folder_path):
        file_path = os.path.join(folder_path, filename)
        if os.path.isdir(file_path):
            load_images_and_labels(file_path, filename)
        elif filename.lower().endswith(('png', 'jpeg', 'jpg')):
            try:
                img = Image.open(file_path)
                images.append(img)
                labels.append(label)
            except Exception as e:
                print(f'Error loading the image {file_path}: {str(e)}')
        else:
            print(f'Skipping non-image file: {file_path}')

for folder_name in os.listdir(master_data_path):
    folder_path = os.path.join(master_data_path, folder_name)
    if os.path.isdir(folder_path):
        load_images_and_labels(folder_path, folder_name)


print(f'Images loaded: {len(images)}')
print(f'Images loaded: {len(labels)}')

import matplotlib.pyplot as plt
import numpy as np

# Convert labels to a 1-dimensional array
labels = np.array(labels)

# Find unique labels
unique_labels = np.unique(labels)

# Define the number of images to display for each unique label
num_images_per_label = 3

# Iterate through unique labels
for label in unique_labels:
    # Find indices of images with the current label
    label_indices = np.where(labels == label)[0]

    # Take the first three indices if available
    label_indices = label_indices[:min(num_images_per_label, len(label_indices))]

    # Create subplots for the current label
    fig, axes = plt.subplots(1, num_images_per_label, figsize=(15, 5))
    fig.suptitle(f'Label: {label}', fontsize=16)

    # Iterate through images corresponding to the current label
    for i, index in enumerate(label_indices):
        # Ensure image is retrieved correctly
        axes[i].imshow(images[index])
        # axes[i].set_title(f'Image {i+1}')
        axes[i].axis('off')

    plt.tight_layout()
    plt.show()

#counting the number of labels in each classes

count_of_classes = {}

for label in labels:
    if label in count_of_classes:
        count_of_classes[label] +=1
    else:
        count_of_classes[label] = 1


for key, value in count_of_classes.items():
    print(f'{key}:{value}')

Dimensions = []

for idx, img in enumerate(images):
    width, height = img.size
    current_dimension = (width, height)
    Dimensions.append(current_dimension)

unique_dimension_count = len(list(set(Dimensions)))

print(f'we have images with {unique_dimension_count} various dimensions')

import numpy as np
from PIL import Image

# Define the target dimension
target_dimension = (299, 299)

# List to store resized images
resized_images = []

# Resize and normalize each image
for img in images:
    # Resize the image
    resized_img = img.resize(target_dimension)

    # Convert to numpy array and normalize
    resized_img_np = np.array(resized_img, dtype=np.float32) / 255.0

    # Append to the list of resized images
    resized_images.append(resized_img_np)

# Convert the list of resized images to numpy array
normalized_resized_images = np.array(resized_images)

# Verify the shape and data type of processed_images
print("Shape of normalized_resized_images:", normalized_resized_images.shape)
print("Data type of normalized_resized_images:", normalized_resized_images.dtype)

# Plot one of the Image
import matplotlib.pyplot as plt

print(labels[0])
plt.imshow(normalized_resized_images[0])
plt.show()

# Find the number of unique values in the dataset
unique_labels = set(list(labels))
num_labels = len(unique_labels)
print(num_labels)

# Encode the labels and store the processed data
import numpy as np
from sklearn.preprocessing import LabelEncoder, OneHotEncoder

# Process images and labels
processed_images = normalized_resized_images
processed_labels = labels

# Encode labels to one-hot
label_encoder = LabelEncoder()
integer_encoded_labels = label_encoder.fit_transform(processed_labels)
onehot_encoder = OneHotEncoder(sparse=False)
onehot_encoded_labels = onehot_encoder.fit_transform(integer_encoded_labels.reshape(-1, 1))

# Print unique labels and their corresponding classes
unique_labels = list(set(processed_labels))
label_classes = label_encoder.transform(unique_labels)
label_class_mapping = dict(zip(unique_labels, label_classes))
print("Label and Class Mapping:")
for label, class_val in label_class_mapping.items():
    print(f"{label}: {class_val}")

# Convert to numpy arrays
processed_images, onehot_encoded_labels = np.array(processed_images), np.array(onehot_encoded_labels)

# Define the path to save the processed data
save_path = "/content/drive/MyDrive/RS LLM/Classification Model/Data Backups/"

# Save processed data
np.save(save_path + "processed_images.npy", processed_images)
np.save(save_path + "onehot_encoded_labels.npy", onehot_encoded_labels)

# Display shapes
print("Shape of processed_images:", processed_images.shape)
print("Shape of onehot_encoded_labels:", onehot_encoded_labels.shape)

"""# Model Development"""

# import numpy as np

# # Define the path to load the processed data
# load_path = "/content/drive/MyDrive/RS LLM/Classification Model/Model and Data Backups/"

# # Load processed data
# processed_images = np.load(load_path + "processed_images.npy")
# onehot_encoded_labels = np.load(load_path + "onehot_encoded_labels.npy")

# # Display shapes
# print("Shape of loaded processed_images:", processed_images.shape)
# print("Shape of loaded onehot_encoded_labels:", onehot_encoded_labels.shape)

import tensorflow as tf
from tensorflow.keras import layers

from tensorflow.keras.applications import InceptionResNetV2

#importing the model & dense layer for customizing the neural network
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, Flatten, BatchNormalization
from tensorflow.keras.optimizers import Adam
from tensorflow.keras import regularizers

#input shape & dimension for the pre-trained models
shape=(299, 299, 3)

base_model = InceptionResNetV2(weights='imagenet', include_top=False, input_shape=shape)
print('Pre-trained model Loaded')

base_model.summary()

#Freezing the pre-trained model's last layer for transfer learning
for layer in base_model.layers:
    layer.trainable=False

"""## Customize the model"""

unique_labels = set(list(labels))
num_labels = len(unique_labels)
print(num_labels)

from tensorflow.keras.layers import Dense, Flatten, BatchNormalization, Dropout

x = base_model.output
x = Flatten()(x)
x = Dense(1024, activation='relu')(x)
x = Dropout(0.25)(x)

x = Dense(512, activation='relu')(x)
x = BatchNormalization()(x)
x = Dropout(0.25)(x)

x = Dense(256, activation='relu')(x)
x = BatchNormalization()(x)
x = Dropout(0.25)(x)

x = Dense(128, activation='relu')(x)
x = BatchNormalization()(x)
x = Dropout(0.25)(x)

x = Dense(64, activation='relu')(x)
x = BatchNormalization()(x)
x = Dropout(0.25)(x)

predictions = Dense(num_labels, activation='softmax')(x)

custom_model = Model(inputs=base_model.input, outputs=predictions)

print("Customized the model")

custom_model.summary()

"""## Compile the model"""

custom_model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])
print("Compiled Custom Model")

"""## Train the model"""

from sklearn.model_selection import train_test_split

# Split the data into training and testing sets with stratified sampling
X_train, X_test, y_train, y_test = train_test_split(
    processed_images,
    onehot_encoded_labels,
    test_size=0.3,
    random_state=42,
    stratify=onehot_encoded_labels  # Add this line for stratified sampling
)

X_test, X_val, y_test, y_val = train_test_split(
    X_test,
    y_test,
    test_size=0.5,
    random_state=42,
    stratify=y_test  # Add this line for stratified sampling
)



# Verify the shapes of the training and testing sets
print("Shape of X_train:", X_train.shape)
print("Shape of y_train:", y_train.shape)
print("Shape of X_test:", X_test.shape)
print("Shape of y_test:", y_test.shape)
print("Shape of X_val:", X_val.shape)
print("Shape of y_val:", y_val.shape)

# Map one-hot encoded labels back to original class labels
mapped_labels_train = label_encoder.inverse_transform(np.argmax(y_train, axis=1))
mapped_labels_test = label_encoder.inverse_transform(np.argmax(y_test, axis=1))
mapped_labels_val = label_encoder.inverse_transform(np.argmax(y_val, axis=1))

# Print the label counts for each set
label_counts_train = dict(zip(*np.unique(mapped_labels_train, return_counts=True)))
label_counts_test = dict(zip(*np.unique(mapped_labels_test, return_counts=True)))
label_counts_val = dict(zip(*np.unique(mapped_labels_val, return_counts=True)))

print("Label counts in training set:", label_counts_train)
print("Label counts in testing set:", label_counts_test)
print("Label counts in validation set:", label_counts_val)

import os
import numpy as np
from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, CSVLogger

def train_and_evaluate_model(X_train, y_train, X_test, y_test, model, model_name, epochs=100):
    """Trains and evaluates the model, saving the best performing model and plotting the confusion matrix.

    Args:
        X_train: Training data features.
        y_train: Training data labels.
        X_test: Testing data features.
        y_test: Testing data labels.
        model: The Keras model to be trained.
        model_name: Name to identify the model.
        epochs: Number of training epochs (default: 100).

    Returns:
        A dictionary containing model name, epoch of best accuracy, and achieved accuracy.
    """

    # Initialize variables for tracking maximum accuracy
    max_accuracy = 0
    max_accuracy_epoch = 0

    # Define callbacks
    model_dir = "/content/drive/MyDrive/RS LLM/Classification Model/Model and Data Backups/Model"
    log_dir = "/content/drive/MyDrive/RS LLM/Classification Model/Model and Data Backups/Logs"
    os.makedirs(model_dir, exist_ok=True)
    os.makedirs(log_dir, exist_ok=True)

    model_path = os.path.join(model_dir, f'{model_name}_full_model.h5')  # Change file extension to .keras
    log_path = os.path.join(log_dir, f'{model_name}_full_training_log.csv')

      checkpoint = ModelCheckpoint(model_path, monitor='val_accuracy',
                                  save_best_only=True, save_weights_only=False, mode='max', verbose=1)
      early_stopping = EarlyStopping(monitor='val_accuracy', patience=10, verbose=1)
      csv_logger = CSVLogger(log_path, append=True)

    # Train the model
    history = model.fit(X_train, y_train, epochs=epochs, validation_data=(X_test, y_test), batch_size=16,
                        callbacks=[checkpoint, early_stopping, csv_logger])

    current_epoch_predictions = []
    current_epoch_labels = []

    # Find the epoch with maximum validation accuracy
    max_accuracy_epoch = np.argmax(history.history['val_accuracy'])
    max_accuracy = history.history['val_accuracy'][max_accuracy_epoch]

    # Evaluate the model on the test data
    test_loss, test_accuracy = model.evaluate(X_test, y_test)
    print(f'Test Accuracy for {model_name}: {test_accuracy}')

    # Print information about the best model
    print("-" * 40)
    print(f"Maximum accuracy of {max_accuracy:.2f} achieved at epoch {max_accuracy_epoch+1}")
    print(f"Model with high accuracy is saved at: {model_path}")
    print(f"Training log is saved at: {log_path}")
    print("-" * 40)
    print("\n")

    # Assuming model.predict returns probabilities for each class
    y_pred_probs = model.predict(X_test)

    # Convert probabilities to class labels
    y_pred = np.argmax(y_pred_probs, axis=1)

    # Convert true labels to class labels if y_test is one-hot encoded
    y_true = np.argmax(y_test, axis=1)

    # Generate confusion matrix
    conf_matrix = confusion_matrix(y_true, y_pred)

    # Plot confusion matrix (assuming class_labels is defined elsewhere)
    plt.figure(figsize=(8, 6))
    sns.heatmap(conf_matrix, annot=True, fmt='g', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.title(model_name)
    plt.show()

    return {'Model': model_name, 'Epoch': max_accuracy_epoch+1, 'Accuracy': max_accuracy}

model_name = "InceptionResNetV2"
class_labels = ['Airport', 'BareLand', 'BaseballField', 'Beach', 'Bridge', 'Center', 'Church', 'Commercial', 'DenseResidential', 'Desert', 'Farmland'
		'Forest', 'Industrial', 'Meadow', 'MediumResidential', 'Mountain', 'Park', 'Parking', 'Playground', 'Pond', 'Port' , 'RailwayStation',
		'Resort', 'River', 'School', 'SparseResidential', 'Square', 'Stadium', 'StorageTanks', 'Viaduct']

result = train_and_evaluate_model(X_train, y_train, X_test, y_test, custom_model, model_name, epochs=100)

print(result)

"""#### Test the model with the validation set but here its test set"""

import os
import tensorflow as tf

directory_path = "/content/drive/MyDrive/RS LLM/Classification Model/Model and Data Backups/Model"
loaded_models = {}

for filename in os.listdir(directory_path):

    if filename.endswith(".h5"):
        model_name = os.path.splitext(filename)[0]
        model_path = os.path.join(directory_path, filename)

        # Print the model path for verification
        print("Attempting to load model:", model_path)

        # Load the model without custom objects
        loaded_model = tf.keras.models.load_model(model_path, custom_objects={})
        loaded_models[model_name] = loaded_model

# Print the loaded model names
print("Loaded model names:")
print(list(loaded_models.keys()))

class_labels = ['Airport', 'BareLand', 'BaseballField', 'Beach', 'Bridge', 'Center', 'Church', 'Commercial', 'DenseResidential', 'Desert', 'Farmland'
		'Forest', 'Industrial', 'Meadow', 'MediumResidential', 'Mountain', 'Park', 'Parking', 'Playground', 'Pond', 'Port' , 'RailwayStation',
		'Resort', 'River', 'School', 'SparseResidential', 'Square', 'Stadium', 'StorageTanks', 'Viaduct']

"""# Test Set"""

# Run the codes after solving the decoder issues.

from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import os

# Initialize an empty list to store results
test_results = []

# Initialize an empty list to store confusion matrices
conf_matrices = []

for model_name, model in loaded_models.items():
    y_pred_probs = model.predict(X_test)
    y_pred = np.argmax(y_pred_probs, axis=1)
    y_true = np.argmax(y_test, axis=1)

    # Calculate accuracy score
    accuracy = accuracy_score(y_true, y_pred)

    # Generate classification report
    class_report = classification_report(y_true, y_pred, target_names=class_labels)

    # Print individual class prediction accuracy score
    print(f"Accuracy for {model_name}: {accuracy:.2f}")
    print(f"Classification Report for {model_name}:\n{class_report}")

    # Append results to the list
    test_results.append([model_name, accuracy])

    # Calculate confusion matrix
    conf_matrix = confusion_matrix(y_true, y_pred)

    # Append confusion matrix to the list
    conf_matrices.append(conf_matrix)

    # Display confusion matrix using seaborn
    plt.figure(figsize=(8, 6))
    sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues", xticklabels=class_labels, yticklabels=class_labels)
    plt.title(f"Confusion Matrix for {model_name} (Test)")
    plt.xlabel("Predicted Label")
    plt.ylabel("True Label")

    # Show the plot
    plt.show()

# Convert the results list to a DataFrame for tabulation
test_results_df = pd.DataFrame(test_results, columns=["Model", "Test Accuracy"])

"""# Validation set"""

from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import os

# Initialize an empty list to store results
val_results = []

# Initialize an empty list to store confusion matrices
conf_matrices = []

for model_name, model in loaded_models.items():
    y_pred_probs = model.predict(X_val)
    y_pred = np.argmax(y_pred_probs, axis=1)
    y_true = np.argmax(y_val, axis=1)

    # Calculate accuracy score
    accuracy = accuracy_score(y_true, y_pred)

    # Generate classification report
    class_report = classification_report(y_true, y_pred, target_names=class_labels)

    # Print individual class prediction accuracy score
    print(f"Accuracy for {model_name}: {accuracy:.2f}")
    print(f"Classification Report for {model_name}:\n{class_report}")

    # Append results to the list
    val_results.append([model_name, accuracy])

    # Calculate confusion matrix
    conf_matrix = confusion_matrix(y_true, y_pred)

    # Append confusion matrix to the list
    conf_matrices.append(conf_matrix)

    # Display confusion matrix using seaborn
    plt.figure(figsize=(8, 6))
    sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues", xticklabels=class_labels, yticklabels=class_labels)
    plt.title(f"Confusion Matrix for {model_name}")
    plt.xlabel("Predicted Label")
    plt.ylabel("True Label")

    # Show the plot
    plt.show()

# Convert the results list to a DataFrame for tabulation
val_results_df = pd.DataFrame(val_results, columns=["Model", "Validation Accuracy"])

"""# The End"""
